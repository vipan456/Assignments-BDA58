{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6k4ZfZSdHqNA",
        "outputId": "9d0ab7e5-3b32-44cb-fbbe-08b7c7667c9b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-1-11b1a797526f>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-11b1a797526f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    get_ipython().system('pip install boto3')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "#Configure AWS Access in Google Colab\n",
        "\n",
        "    !pip install boto3\n",
        "    import boto3\n",
        "\n",
        "    # Configure AWS Credentials\n",
        "    aws_access_key = \"YOUR_AWS_ACCESS_KEY\"\n",
        "    aws_secret_key = \"YOUR_AWS_SECRET_KEY\"\n",
        "    region_name = \"YOUR_AWS_REGION\"\n",
        "\n",
        "    session = boto3.Session(\n",
        "        aws_access_key_id=aws_access_key,\n",
        "        aws_secret_access_key=aws_secret_key,\n",
        "        region_name=region_name\n",
        "    )\n",
        "    s3 = session.client('s3')\n",
        "    print(\"AWS session configured successfully.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2: Collect Amazon Product Reviews\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Task 2: Collect Amazon Product Reviews\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the CSV file from your Google Drive\n",
        "# Make sure to replace 'path/to/your/file/datasettt.csv' with the actual path to your CSV file on your Drive\n",
        "file_path = '/content/drive/MyDrive/datasettt.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qeWffctIATb",
        "outputId": "64a9a7e0-ef51-49fe-a3de-469091e3b3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                            uniq_id  \\\n",
            "0  eac7efa5dbd3d667f26eb3d3ab504464   \n",
            "1  b17540ef7e86e461d37f3ae58b7b72ac   \n",
            "2  348f344247b0c1a935b1223072ef9d8a   \n",
            "3  e12b92dbb8eaee78b22965d2a9bbbd9f   \n",
            "4  e33a9adeed5f36840ccc227db4682a36   \n",
            "\n",
            "                                        product_name manufacturer   price  \\\n",
            "0                              Hornby 2014 Catalogue       Hornby   £3.42   \n",
            "1  FunkyBuys® Large Christmas Holiday Express Fes...    FunkyBuys  £16.99   \n",
            "2  CLASSIC TOY TRAIN SET TRACK CARRIAGES LIGHT EN...          ccf   £9.99   \n",
            "3     HORNBY Coach R4410A BR Hawksworth Corridor 3rd       Hornby  £39.99   \n",
            "4  Hornby 00 Gauge 0-4-0 Gildenlow Salt Co. Steam...       Hornby  £32.19   \n",
            "\n",
            "  number_available_in_stock number_of_reviews  number_of_answered_questions  \\\n",
            "0                     5 new                15                           1.0   \n",
            "1                       NaN                 2                           1.0   \n",
            "2                     2 new                17                           2.0   \n",
            "3                       NaN                 1                           2.0   \n",
            "4                       NaN                 3                           2.0   \n",
            "\n",
            "  average_review_rating                   amazon_category_and_sub_category  \\\n",
            "0    4.9 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
            "1    4.5 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
            "2    3.9 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
            "3    5.0 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
            "4    4.7 out of 5 stars  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
            "\n",
            "          customers_who_bought_this_item_also_bought  \\\n",
            "0  http://www.amazon.co.uk/Hornby-R8150-Catalogue...   \n",
            "1  http://www.amazon.co.uk/Christmas-Holiday-Expr...   \n",
            "2  http://www.amazon.co.uk/Classic-Train-Lights-B...   \n",
            "3                                                NaN   \n",
            "4  http://www.amazon.co.uk/Hornby-R6367-RailRoad-...   \n",
            "\n",
            "                                         description  \\\n",
            "0  Product Description Hornby 2014 Catalogue Box ...   \n",
            "1  Size Name:Large FunkyBuys® Large Christmas Hol...   \n",
            "2  BIG CLASSIC TOY TRAIN SET TRACK CARRIAGE LIGHT...   \n",
            "3  Hornby 00 Gauge BR Hawksworth 3rd Class W 2107...   \n",
            "4  Product Description Hornby RailRoad 0-4-0 Gild...   \n",
            "\n",
            "                                 product_information  \\\n",
            "0  Technical Details Item Weight640 g Product Dim...   \n",
            "1  Technical Details Manufacturer recommended age...   \n",
            "2  Technical Details Manufacturer recommended age...   \n",
            "3  Technical Details Item Weight259 g Product Dim...   \n",
            "4  Technical Details Item Weight159 g Product Dim...   \n",
            "\n",
            "                                 product_description  \\\n",
            "0  Product Description Hornby 2014 Catalogue Box ...   \n",
            "1  Size Name:Large FunkyBuys® Large Christmas Hol...   \n",
            "2  BIG CLASSIC TOY TRAIN SET TRACK CARRIAGE LIGHT...   \n",
            "3  Hornby 00 Gauge BR Hawksworth 3rd Class W 2107...   \n",
            "4  Product Description Hornby RailRoad 0-4-0 Gild...   \n",
            "\n",
            "         items_customers_buy_after_viewing_this_item  \\\n",
            "0  http://www.amazon.co.uk/Hornby-R8150-Catalogue...   \n",
            "1  http://www.amazon.co.uk/Christmas-Holiday-Expr...   \n",
            "2  http://www.amazon.co.uk/Train-With-Tracks-Batt...   \n",
            "3                                                NaN   \n",
            "4  http://www.amazon.co.uk/Hornby-R2672-RailRoad-...   \n",
            "\n",
            "                      customer_questions_and_answers  \\\n",
            "0  Does this catalogue detail all the previous Ho...   \n",
            "1  can you turn off sounds // hi no you cant turn...   \n",
            "2  What is the gauge of the track // Hi Paul.Trut...   \n",
            "3                                                NaN   \n",
            "4                                                NaN   \n",
            "\n",
            "                                    customer_reviews  \\\n",
            "0  Worth Buying For The Pictures Alone (As Ever) ...   \n",
            "1  Four Stars // 4.0 // 18 Dec. 2015 // By\\n    \\...   \n",
            "2  **Highly Recommended!** // 5.0 // 26 May 2015 ...   \n",
            "3  I love it // 5.0 // 22 July 2013 // By\\n    \\n...   \n",
            "4  Birthday present // 5.0 // 14 April 2014 // By...   \n",
            "\n",
            "                                             sellers  \n",
            "0  {\"seller\"=>[{\"Seller_name_1\"=>\"Amazon.co.uk\", ...  \n",
            "1  {\"seller\"=>{\"Seller_name_1\"=>\"UHD WHOLESALE\", ...  \n",
            "2  {\"seller\"=>[{\"Seller_name_1\"=>\"DEAL-BOX\", \"Sel...  \n",
            "3                                                NaN  \n",
            "4                                                NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the URL to scrape\n",
        "url = \"https://www.amazon.com/some-product-url\"\n",
        "\n",
        "# Headers to mimic a real browser visit\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US, en;q=0.5\"\n",
        "}\n",
        "\n",
        "# Send an HTTP request to the URL\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Parse the content of the request with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Define a function to extract product reviews\n",
        "def extract_reviews(soup):\n",
        "    reviews = []\n",
        "    for review in soup.find_all(\"span\", class_=\"a-size-base review-text review-text-content\"):\n",
        "        reviews.append(review.get_text(strip=True))\n",
        "    return reviews\n",
        "\n",
        "# Extract reviews from the page\n",
        "reviews = extract_reviews(soup)\n",
        "\n",
        "# Convert the reviews into a DataFrame\n",
        "df = pd.DataFrame(reviews, columns=[\"Review\"])\n",
        "\n",
        "# Save the DataFrame to your Google Drive\n",
        "file_path = '/content/drive/MyDrive/datasettt.csv'\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Load the CSV file from your Google Drive and display the first few rows\n",
        "df_loaded = pd.read_csv(file_path)\n",
        "print(df_loaded.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO-64TlWQh_7",
        "outputId": "fae3dfa1-f263-49a0-b850-d1735c678cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Empty DataFrame\n",
            "Columns: [Review]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Clean and Preprocess Data\n",
        "#Check for Missing Values and Basic Information\n",
        "# Check for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhfh4x1fT_PK",
        "outputId": "ad1ed265-2437-4f1b-b295-f553d010abc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Review    0\n",
            "dtype: int64\n",
            "\n",
            "Data types of each column:\n",
            "Review    object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle Missing Values\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Alternatively, fill missing values (if necessary)\n",
        "# df['Column_Name'] = df['Column_Name'].fillna('Default Value')\n",
        "print(\"\\nMissing values handled successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT27qvoaUR28",
        "outputId": "1b882dfd-71ab-4966-97a7-0ac7a55d3471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values handled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the URL to scrape\n",
        "url = \"https://www.amazon.com/some-product-url\"\n",
        "\n",
        "# Headers to mimic a real browser visit\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US, en;q=0.5\"\n",
        "}\n",
        "\n",
        "# Send an HTTP request to the URL\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Parse the content of the request with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Define a function to extract product reviews\n",
        "def extract_reviews(soup):\n",
        "    reviews = []\n",
        "    for review in soup.find_all(\"span\", class_=\"a-size-base review-text review-text-content\"):\n",
        "        reviews.append(review.get_text(strip=True))\n",
        "    return reviews\n",
        "\n",
        "# Extract reviews from the page\n",
        "reviews = extract_reviews(soup)\n",
        "\n",
        "# Convert the reviews into a DataFrame\n",
        "df = pd.DataFrame(reviews, columns=[\"Review\"])\n",
        "\n",
        "# Save the DataFrame to your Google Drive\n",
        "file_path = '/content/drive/MyDrive/datasettt.csv'\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Load the CSV file from your Google Drive and display the first few rows\n",
        "df_loaded = pd.read_csv(file_path)\n",
        "print(df_loaded.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae3dfa1-f263-49a0-b850-d1735c678cf7",
        "id": "AEhhedX3X9Y8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Empty DataFrame\n",
            "Columns: [Review]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Clean and Preprocess Data\n",
        "#Check for Missing Values and Basic Information\n",
        "# Check for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1ed265-2437-4f1b-b295-f553d010abc4",
        "id": "MDzuqLtcX9ZF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Review    0\n",
            "dtype: int64\n",
            "\n",
            "Data types of each column:\n",
            "Review    object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle Missing Values\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Alternatively, fill missing values (if necessary)\n",
        "# df['Column_Name'] = df['Column_Name'].fillna('Default Value')\n",
        "print(\"\\nMissing values handled successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b882dfd-71ab-4966-97a7-0ac7a55d3471",
        "id": "4P8OV4l9X9ZF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values handled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the URL to scrape\n",
        "url = \"https://www.amazon.com/some-product-url\"\n",
        "\n",
        "# Headers to mimic a real browser visit\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US, en;q=0.5\"\n",
        "}\n",
        "\n",
        "# Send an HTTP request to the URL\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Parse the content of the request with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Define a function to extract product reviews\n",
        "def extract_reviews(soup):\n",
        "    reviews = []\n",
        "    for review in soup.find_all(\"span\", class_=\"a-size-base review-text review-text-content\"):\n",
        "        reviews.append(review.get_text(strip=True))\n",
        "    return reviews\n",
        "\n",
        "# Extract reviews from the page\n",
        "reviews = extract_reviews(soup)\n",
        "\n",
        "# Convert the reviews into a DataFrame\n",
        "df = pd.DataFrame(reviews, columns=[\"Review\"])\n",
        "\n",
        "# Save the DataFrame to your Google Drive\n",
        "file_path = '/content/drive/MyDrive/datasettt.csv'\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Load the CSV file from your Google Drive and display the first few rows\n",
        "df_loaded = pd.read_csv(file_path)\n",
        "print(df_loaded.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae3dfa1-f263-49a0-b850-d1735c678cf7",
        "id": "bYSBwY6mX-uS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Empty DataFrame\n",
            "Columns: [Review]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Clean and Preprocess Data\n",
        "#Check for Missing Values and Basic Information\n",
        "# Check for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1ed265-2437-4f1b-b295-f553d010abc4",
        "id": "s7qNVKhAX-uZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Review    0\n",
            "dtype: int64\n",
            "\n",
            "Data types of each column:\n",
            "Review    object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle Missing Values\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Alternatively, fill missing values (if necessary)\n",
        "# df['Column_Name'] = df['Column_Name'].fillna('Default Value')\n",
        "print(\"\\nMissing values handled successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b882dfd-71ab-4966-97a7-0ac7a55d3471",
        "id": "E1IozxLLX-ua"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values handled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Clean and Preprocess Data\n",
        "#Check for Missing Values and Basic Information\n",
        "# Check for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "#Handle Missing Values\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Alternatively, fill missing values (if necessary)\n",
        "# df['Column_Name'] = df['Column_Name'].fillna('Default Value')\n",
        "print(\"\\nMissing values handled successfully.\")\n",
        "\n",
        "#Data Preprocessing and Text Cleaning\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the cleaning function to the reviews\n",
        "# Assuming your DataFrame has a column named 'Review' containing the raw reviews\n",
        "# Replace 'Review' with the actual column name if it's different\n",
        "df['Cleaned_Review'] = df['Review'].apply(lambda x: clean_text(str(x)))\n",
        "print(\"\\nText data cleaned successfully. Here are some samples:\")\n",
        "print(df[['Review', 'Cleaned_Review']].head()) # Replace 'Review' if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAsYYdZBU3me",
        "outputId": "a48d9f67-757e-4168-f93e-5233a5d43294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Review    0\n",
            "dtype: int64\n",
            "\n",
            "Data types of each column:\n",
            "Review    object\n",
            "dtype: object\n",
            "\n",
            "Missing values handled successfully.\n",
            "\n",
            "Text data cleaned successfully. Here are some samples:\n",
            "Empty DataFrame\n",
            "Columns: [Review, Cleaned_Review]\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Milestone 2: Model Selection and Fine-tuning\n",
        "#Step 4: Install Libraries for Model Training\n",
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOodsPFAU7e5",
        "outputId": "6f1815d9-c9e8-416f-a143-beef590bc10e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Clean and Preprocess Data\n",
        "#Check for Missing Values and Basic Information\n",
        "# Check for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "#Handle Missing Values\n",
        "# Instead of dropping all rows with missing values,\n",
        "# consider filling them or dropping only rows where 'Sentiment' is missing\n",
        "# Check if 'Sentiment' column exists before handling missing values:\n",
        "if 'Sentiment' in df.columns:\n",
        "    # Option 1: Drop rows only where 'Sentiment' is missing:\n",
        "    df = df.dropna(subset=['Sentiment'])\n",
        "\n",
        "    # Option 2: Fill missing 'Sentiment' values with a default value (e.g., 'Unknown'):\n",
        "    # df['Sentiment'] = df['Sentiment'].fillna('Unknown')\n",
        "else:\n",
        "    print(\"Warning: 'Sentiment' column not found in the DataFrame.\")\n",
        "\n",
        "\n",
        "# Alternatively, fill missing values (if necessary)\n",
        "# df['Column_Name'] = df['Column_Name'].fillna('Default Value')\n",
        "print(\"\\nMissing values handled successfully.\")\n",
        "\n",
        "#Data Preprocessing and Text Cleaning\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the cleaning function to the reviews\n",
        "# Assuming your DataFrame has a column named 'Review' containing the raw reviews\n",
        "# Replace 'Review' with the actual column name if it's different\n",
        "# Check if 'Review' column exists before applying cleaning function:\n",
        "if 'Review' in df.columns:\n",
        "    df['Cleaned_Review'] = df['Review'].apply(lambda x: clean_text(str(x)))\n",
        "    print(\"\\nText data cleaned successfully. Here are some samples:\")\n",
        "    print(df[['Review', 'Cleaned_Review']].head()) # Replace 'Review' if needed\n",
        "else:\n",
        "    print(\"Warning: 'Review' column not found in the DataFrame.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgwY-SUEV8LY",
        "outputId": "875821df-4d8e-4c35-db1c-14a4e34e2824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Review            0\n",
            "Cleaned_Review    0\n",
            "dtype: int64\n",
            "\n",
            "Data types of each column:\n",
            "Review            object\n",
            "Cleaned_Review    object\n",
            "dtype: object\n",
            "Warning: 'Sentiment' column not found in the DataFrame.\n",
            "\n",
            "Missing values handled successfully.\n",
            "\n",
            "Text data cleaned successfully. Here are some samples:\n",
            "Empty DataFrame\n",
            "Columns: [Review, Cleaned_Review]\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Assuming df is your DataFrame containing the reviews loaded from Google Drive\n",
        "# Let's check if the 'Sentiment' column exists\n",
        "if 'Sentiment' not in df.columns:\n",
        "    print(\"The 'Sentiment' column was not found in the DataFrame. Please ensure your data includes sentiment labels.\")\n",
        "    # For demonstration purposes, we'll create a dummy 'Sentiment' column\n",
        "    # This can be replaced with your actual sentiment labels\n",
        "    df['Sentiment'] = ['positive', 'neutral', 'negative'] * (len(df) // 3 + 1)\n",
        "    df = df[:len(df)]  # Adjust the length to match the original DataFrame\n",
        "\n",
        "# Map sentiment labels to numeric values (e.g., positive -> 2, neutral -> 1, negative -> 0)\n",
        "label_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "df['Sentiment_Label'] = df['Sentiment'].map(label_mapping)\n",
        "\n",
        "# Check the updated DataFrame\n",
        "print(\"DataFrame with Sentiment labels:\")\n",
        "print(df[['Sentiment', 'Sentiment_Label']].head())\n",
        "\n",
        "# Tokenize the cleaned text reviews using BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Replace NaN values in 'Cleaned_Review' with an empty string\n",
        "df['Cleaned_Review'] = df['Cleaned_Review'].fillna('')\n",
        "\n",
        "# Tokenize and encode the reviews\n",
        "encoded_reviews = [\n",
        "    tokenizer.encode(review, truncation=True, padding='max_length', max_length=512)\n",
        "    for review in df['Cleaned_Review']\n",
        "]\n",
        "\n",
        "# Define the PyTorch Dataset class\n",
        "class AmazonReviewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, reviews, labels):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = self.reviews[idx]\n",
        "        label = self.labels[idx]\n",
        "        return {\n",
        "            'review': torch.tensor(review, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create the PyTorch Dataset\n",
        "reviews_dataset = AmazonReviewsDataset(encoded_reviews, df['Sentiment_Label'].tolist())\n",
        "print(\"PyTorch Dataset created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgYVMzX9aiq9",
        "outputId": "0165f9bd-cf9e-4fe3-ffb0-473b3cbcefc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-726d99da7075>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Sentiment_Label'] = df['Sentiment'].map(label_mapping)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with Sentiment labels:\n",
            "  Sentiment  Sentiment_Label\n",
            "0  positive                2\n",
            "1   neutral                1\n",
            "2  negative                0\n",
            "PyTorch Dataset created successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "<ipython-input-44-726d99da7075>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Cleaned_Review'] = df['Cleaned_Review'].fillna('')\n"
          ]
        }
      ]
    }
  ]
}