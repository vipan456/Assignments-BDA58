{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c4da56-0e96-4224-872d-c231981f2b3d",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "The objective of this assignment is to help trainees gain hands-on experience with Scrapy, a powerful web scraping framework in Python. By the end of this assignment, trainees should be able to create Scrapy projects, build spiders to extract data from websites, and store the scraped data in various formats.\n",
    "\n",
    "Task 1: Install and Set Up Scrapy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db676d7-b84f-4daa-8dd7-223b47e2fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\vipan\\anaconda3\\lib\\site-packages (2.11.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (42.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (24.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (69.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (23.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (5.2.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.11.0)\n",
      "Requirement already satisfied: idna in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.32.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vipan\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "383c1350-4b05-40ab-a306-ae1735521ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eeb78a-e2b0-43be-b920-2984cadbc129",
   "metadata": {},
   "source": [
    "Create a Scrapy Project:\n",
    "Create a new Scrapy project named \"web_scraper\" in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a8497-180f-465c-9e18-60d0514ce542",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject web_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27deb83b-8c23-4d32-ae6d-73332f2f2d49",
   "metadata": {},
   "source": [
    "Task 2: Create a Spider to Scrape a Website\n",
    "Choose a Website: Select a simple, publicly accessible website to scrape.\n",
    "Examples include:\n",
    "\n",
    "http://quotes.toscrape.com (A website designed for practicing web scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18874bfd-b1e5-468d-ad08-834a8cc4d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "            next_page = response.css('li.nexta::attr(href)').get()\n",
    "            if next_page is not None:\n",
    "             yield\n",
    "             response.follow(next_page, self.parse)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5170c31-0086-4257-b1f5-4522f5c35a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl quotes # this is not doing anything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebda234-02d7-4114-844b-8c2ed4b7b0f9",
   "metadata": {},
   "source": [
    "Task 3: Save the Scraped Data\n",
    "Save Data to a JSON File: Run the spider and save the scraped data to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d921b1-b36f-477b-b0db-0b8ce3b95591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
